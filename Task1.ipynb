{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e9c910-dd58-40ed-8bdc-1db78dca9fe8",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f89fb88-0dfa-4c32-bd82-4dc4ac33e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65543cb-12e1-42f6-89a3-8e8764bd1315",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1cecdae-85b9-4fe8-a395-2e298bb1e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c91e5-d9e9-4905-8887-e1d53320f988",
   "metadata": {},
   "source": [
    "Data Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b6c8fd8-8b90-45c6-aa93-82e294e5d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1798c1-d975-41c5-af9e-ac2322f6df0b",
   "metadata": {},
   "source": [
    "Print shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9ef3249-6f0d-4f08-b9a2-3aa24bd1d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (60000, 784)\n",
      "Train Labels Shape: (60000,)\n",
      "Test Images Shape: (10000, 784)\n",
      "Test Labels Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Images Shape: {x_train.shape}')\n",
    "print(f'Train Labels Shape: {y_train.shape}')\n",
    "print(f'Test Images Shape: {x_test.shape}')\n",
    "print(f'Test Labels Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f95f799-1f9a-4732-9c16-8946e3f63823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_images(images, labels, num_images=5):\n",
    "    images = images.reshape((-1, 28, 28))\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(\"Label: {}\".format(np.argmax(labels[i])))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_images(x_train, y_train, num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff4bd7c9-2051-45d6-b1e0-1f26f862ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_functions, dropout_rate=0.0, regularization=None, lambd=0.0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.activation_functions = activation_functions\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = {\n",
    "                'weights': np.random.randn(hidden_size, prev_size) / np.sqrt(prev_size),\n",
    "                'biases': np.random.randn(hidden_size, 1) / np.sqrt(hidden_size)\n",
    "            }\n",
    "            self.layers.append(layer)\n",
    "            prev_size = hidden_size\n",
    "        self.output_layer = {\n",
    "            'weights': np.random.randn(output_size, prev_size) / np.sqrt(prev_size),\n",
    "            'biases': np.random.randn(output_size, 1) / np.sqrt(prev_size)\n",
    "        }\n",
    "\n",
    "    def activation_function(self, Z, func_type='ReLU', derivative=False):\n",
    "        if func_type == 'ReLU':\n",
    "            if derivative:\n",
    "                return (Z > 0).astype(float)\n",
    "            return np.maximum(0, Z)\n",
    "        elif func_type == 'Sigmoid':\n",
    "            sig = 1 / (1 + np.exp(-Z))\n",
    "            if derivative:\n",
    "                return sig * (1 - sig)\n",
    "            return sig\n",
    "        elif func_type == 'tanh':\n",
    "            tanh = np.tanh(Z)\n",
    "            if derivative:\n",
    "                return 1 - np.square(tanh)\n",
    "            return tanh\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation function type.')\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))\n",
    "        return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, labels):\n",
    "        return -np.log(predictions[labels])\n",
    "\n",
    "    def forward_pass(self, x, y, apply_dropout=False):\n",
    "        activations = [x.reshape(-1, 1)]\n",
    "        dropout_masks = []\n",
    "        for layer, func in zip(self.layers, self.activation_functions):\n",
    "            Z = np.dot(layer['weights'], activations[-1]) + layer['biases']\n",
    "            A = self.activation_function(Z, func)\n",
    "            if apply_dropout and self.dropout_rate > 0:\n",
    "                dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, size=A.shape) / (1 - self.dropout_rate)\n",
    "                A *= dropout_mask\n",
    "                dropout_masks.append(dropout_mask)\n",
    "            activations.append(A)\n",
    "        Z = np.dot(self.output_layer['weights'], activations[-1]) + self.output_layer['biases']\n",
    "        predictions = self.softmax(Z).squeeze()\n",
    "        error = self.cross_entropy_loss(predictions, y)\n",
    "        return {'activations': activations, 'predictions': predictions, 'error': error, 'dropout_masks': dropout_masks}\n",
    "\n",
    "    def back_propagation(self, x, y, forward_results):\n",
    "        E = np.zeros(self.output_size).reshape((1, self.output_size))\n",
    "        E[0, y] = 1\n",
    "        dU = -(E - forward_results['predictions']).reshape((self.output_size, 1))\n",
    "        dC = np.dot(dU, forward_results['activations'][-1].T)\n",
    "        delta = np.dot(self.output_layer['weights'].T, dU)\n",
    "        deltas = [delta]\n",
    "        grads = [{'weights': dC, 'biases': dU}]\n",
    "\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            activation_func = self.activation_functions[i]\n",
    "            A_prev = forward_results['activations'][i + 1]\n",
    "            dA = deltas[-1]\n",
    "            if self.dropout_rate > 0 and i < len(forward_results['dropout_masks']):\n",
    "                dA *= forward_results['dropout_masks'][i]\n",
    "            dZ = dA * self.activation_function(A_prev, activation_func, derivative=True)\n",
    "            dW = np.dot(dZ, forward_results['activations'][i].T)\n",
    "            db = dZ\n",
    "            delta = np.dot(self.layers[i]['weights'].T, dZ)\n",
    "            deltas.append(delta)\n",
    "            grads.append({'weights': dW, 'biases': db})\n",
    "\n",
    "        return grads[::-1]\n",
    "\n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.regularization == 'L2':\n",
    "                layer['weights'] -= learning_rate * (grads[i]['weights'] + self.lambd * layer['weights'])\n",
    "            elif self.regularization == 'L1':\n",
    "                layer['weights'] -= learning_rate * (grads[i]['weights'] + self.lambd * np.sign(layer['weights']))\n",
    "            else:\n",
    "                layer['weights'] -= learning_rate * grads[i]['weights']\n",
    "            layer['biases'] -= learning_rate * grads[i]['biases']\n",
    "        self.output_layer['weights'] -= learning_rate * grads[-1]['weights']\n",
    "        self.output_layer['biases'] -= learning_rate * grads[-1]['biases']\n",
    "\n",
    "    def compute_loss(self, X, Y):\n",
    "        total_loss = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            total_loss += self.forward_pass(x, y)['error']\n",
    "        if self.regularization == 'L2':\n",
    "            total_loss += self.lambd / 2 * sum(np.sum(layer['weights'] ** 2) for layer in self.layers)\n",
    "        elif self.regularization == 'L1':\n",
    "            total_loss += self.lambd * sum(np.sum(np.abs(layer['weights'])) for layer in self.layers)\n",
    "        return total_loss / len(Y)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_test, y_test, num_iterations=1000, learning_rate=0.5, batch_size=32, optimizer=None):\n",
    "        training_losses = []\n",
    "        test_accuracies = []\n",
    "        training_accuracies = []\n",
    "        testing_losses = []\n",
    "        initial_lr = learning_rate\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_indices = np.random.choice(len(X_train), batch_size)\n",
    "            X_batch = X_train[batch_indices]\n",
    "            Y_batch = Y_train[batch_indices]\n",
    "\n",
    "            for x, y in zip(X_batch, Y_batch):\n",
    "                forward_results = self.forward_pass(x, y, apply_dropout=True)\n",
    "                grads = self.back_propagation(x, y, forward_results)\n",
    "                self.update_parameters(grads, learning_rate)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                train_loss = self.compute_loss(X_train, Y_train)\n",
    "                train_accuracy, _ = self.evaluate(X_train, Y_train)\n",
    "                test_accuracy, _ = self.evaluate(X_test, y_test)\n",
    "                test_loss = self.compute_loss(X_test, y_test)\n",
    "\n",
    "                training_losses.append(train_loss)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                training_accuracies.append(train_accuracy)\n",
    "                testing_losses.append(test_loss)\n",
    "\n",
    "                print(f'Iteration {i}: Training Loss = {train_loss:.4f}, Training Accuracy = {train_accuracy:.4f}, Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}')\n",
    "\n",
    "                # Learning rate decay\n",
    "                learning_rate = initial_lr * (0.95 ** (i // 1000))\n",
    "\n",
    "        return training_losses, test_accuracies, training_accuracies, testing_losses\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        correct_predictions = 0\n",
    "        predictions = []\n",
    "        for x, y in zip(X, Y):\n",
    "            forward_results = self.forward_pass(x, y)\n",
    "            prediction = np.argmax(forward_results['predictions'])\n",
    "            predictions.append(prediction)\n",
    "            if prediction == y:\n",
    "                correct_predictions += 1\n",
    "        accuracy = correct_predictions / len(Y)\n",
    "        return accuracy, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40e08576-f75d-4083-b19c-36e76f61c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 28 * 28\n",
    "num_outputs = 10\n",
    "hidden_sizes = [300]\n",
    "activation_functions = ['ReLU']\n",
    "num_iterations = 5000\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "lambd = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60059664-2eac-40ed-b574-306e4062d68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Training Loss = 2.1920, Training Accuracy = 0.2815, Test Loss = 2.1944, Test Accuracy = 0.2786\n",
      "Iteration 100: Training Loss = 0.4455, Training Accuracy = 0.8749, Test Loss = 0.4256, Test Accuracy = 0.8845\n",
      "Iteration 200: Training Loss = 0.3799, Training Accuracy = 0.8832, Test Loss = 0.3643, Test Accuracy = 0.8934\n",
      "Iteration 300: Training Loss = 0.3542, Training Accuracy = 0.8960, Test Loss = 0.3388, Test Accuracy = 0.9027\n",
      "Iteration 400: Training Loss = 0.3536, Training Accuracy = 0.8938, Test Loss = 0.3562, Test Accuracy = 0.8913\n",
      "Iteration 500: Training Loss = 0.3028, Training Accuracy = 0.9073, Test Loss = 0.2889, Test Accuracy = 0.9120\n",
      "Iteration 600: Training Loss = 0.2982, Training Accuracy = 0.9090, Test Loss = 0.2857, Test Accuracy = 0.9154\n",
      "Iteration 700: Training Loss = 0.2570, Training Accuracy = 0.9247, Test Loss = 0.2447, Test Accuracy = 0.9249\n",
      "Iteration 800: Training Loss = 0.2821, Training Accuracy = 0.9112, Test Loss = 0.2687, Test Accuracy = 0.9169\n",
      "Iteration 900: Training Loss = 0.2712, Training Accuracy = 0.9192, Test Loss = 0.2582, Test Accuracy = 0.9216\n",
      "Iteration 1000: Training Loss = 0.2838, Training Accuracy = 0.9124, Test Loss = 0.2709, Test Accuracy = 0.9126\n",
      "Iteration 1100: Training Loss = 0.2603, Training Accuracy = 0.9180, Test Loss = 0.2434, Test Accuracy = 0.9242\n",
      "Iteration 1200: Training Loss = 0.2518, Training Accuracy = 0.9229, Test Loss = 0.2524, Test Accuracy = 0.9218\n",
      "Iteration 1300: Training Loss = 0.3016, Training Accuracy = 0.9065, Test Loss = 0.3023, Test Accuracy = 0.9034\n",
      "Iteration 1400: Training Loss = 0.2576, Training Accuracy = 0.9208, Test Loss = 0.2580, Test Accuracy = 0.9222\n",
      "Iteration 1500: Training Loss = 0.2697, Training Accuracy = 0.9165, Test Loss = 0.2543, Test Accuracy = 0.9228\n",
      "Iteration 1600: Training Loss = 0.2582, Training Accuracy = 0.9229, Test Loss = 0.2452, Test Accuracy = 0.9260\n",
      "Iteration 1700: Training Loss = 0.2430, Training Accuracy = 0.9275, Test Loss = 0.2349, Test Accuracy = 0.9299\n",
      "Iteration 1800: Training Loss = 0.3312, Training Accuracy = 0.8921, Test Loss = 0.3293, Test Accuracy = 0.8897\n",
      "Iteration 1900: Training Loss = 0.2472, Training Accuracy = 0.9280, Test Loss = 0.2368, Test Accuracy = 0.9295\n",
      "Iteration 2000: Training Loss = 0.2240, Training Accuracy = 0.9341, Test Loss = 0.2076, Test Accuracy = 0.9385\n",
      "Iteration 2100: Training Loss = 0.2289, Training Accuracy = 0.9315, Test Loss = 0.2249, Test Accuracy = 0.9339\n",
      "Iteration 2200: Training Loss = 0.2963, Training Accuracy = 0.9079, Test Loss = 0.2941, Test Accuracy = 0.9076\n",
      "Iteration 2300: Training Loss = 0.2605, Training Accuracy = 0.9216, Test Loss = 0.2500, Test Accuracy = 0.9248\n",
      "Iteration 2400: Training Loss = 0.2339, Training Accuracy = 0.9327, Test Loss = 0.2268, Test Accuracy = 0.9316\n",
      "Iteration 2500: Training Loss = 0.2365, Training Accuracy = 0.9288, Test Loss = 0.2336, Test Accuracy = 0.9292\n",
      "Iteration 2600: Training Loss = 0.2350, Training Accuracy = 0.9295, Test Loss = 0.2342, Test Accuracy = 0.9275\n",
      "Iteration 2700: Training Loss = 0.2176, Training Accuracy = 0.9345, Test Loss = 0.2147, Test Accuracy = 0.9343\n",
      "Iteration 2800: Training Loss = 0.2361, Training Accuracy = 0.9282, Test Loss = 0.2284, Test Accuracy = 0.9299\n",
      "Iteration 2900: Training Loss = 0.2203, Training Accuracy = 0.9300, Test Loss = 0.2175, Test Accuracy = 0.9334\n",
      "Iteration 3000: Training Loss = 0.2959, Training Accuracy = 0.9052, Test Loss = 0.2798, Test Accuracy = 0.9118\n",
      "Iteration 3100: Training Loss = 0.2497, Training Accuracy = 0.9245, Test Loss = 0.2537, Test Accuracy = 0.9232\n",
      "Iteration 3200: Training Loss = 0.2055, Training Accuracy = 0.9391, Test Loss = 0.2003, Test Accuracy = 0.9395\n",
      "Iteration 3300: Training Loss = 0.2247, Training Accuracy = 0.9321, Test Loss = 0.2237, Test Accuracy = 0.9331\n",
      "Iteration 3400: Training Loss = 0.2765, Training Accuracy = 0.9062, Test Loss = 0.2647, Test Accuracy = 0.9109\n",
      "Iteration 3500: Training Loss = 0.2783, Training Accuracy = 0.9151, Test Loss = 0.2760, Test Accuracy = 0.9164\n",
      "Iteration 3600: Training Loss = 0.2225, Training Accuracy = 0.9324, Test Loss = 0.2254, Test Accuracy = 0.9319\n",
      "Iteration 3700: Training Loss = 0.2487, Training Accuracy = 0.9240, Test Loss = 0.2416, Test Accuracy = 0.9260\n",
      "Iteration 3800: Training Loss = 0.2469, Training Accuracy = 0.9194, Test Loss = 0.2427, Test Accuracy = 0.9178\n",
      "Iteration 3900: Training Loss = 0.2592, Training Accuracy = 0.9206, Test Loss = 0.2572, Test Accuracy = 0.9214\n",
      "Iteration 4000: Training Loss = 0.2346, Training Accuracy = 0.9280, Test Loss = 0.2330, Test Accuracy = 0.9255\n",
      "Iteration 4100: Training Loss = 0.2273, Training Accuracy = 0.9316, Test Loss = 0.2252, Test Accuracy = 0.9301\n",
      "Iteration 4200: Training Loss = 0.2040, Training Accuracy = 0.9392, Test Loss = 0.1976, Test Accuracy = 0.9395\n",
      "Iteration 4300: Training Loss = 0.2243, Training Accuracy = 0.9312, Test Loss = 0.2278, Test Accuracy = 0.9302\n",
      "Iteration 4400: Training Loss = 0.2327, Training Accuracy = 0.9271, Test Loss = 0.2237, Test Accuracy = 0.9292\n",
      "Iteration 4500: Training Loss = 0.2238, Training Accuracy = 0.9366, Test Loss = 0.2182, Test Accuracy = 0.9378\n",
      "Iteration 4600: Training Loss = 0.2200, Training Accuracy = 0.9337, Test Loss = 0.2146, Test Accuracy = 0.9324\n",
      "Iteration 4700: Training Loss = 0.2379, Training Accuracy = 0.9273, Test Loss = 0.2366, Test Accuracy = 0.9231\n",
      "Iteration 4800: Training Loss = 0.2109, Training Accuracy = 0.9382, Test Loss = 0.2052, Test Accuracy = 0.9384\n",
      "Iteration 4900: Training Loss = 0.2174, Training Accuracy = 0.9325, Test Loss = 0.2096, Test Accuracy = 0.9338\n"
     ]
    }
   ],
   "source": [
    "nn_model = NeuralNetwork(num_inputs, hidden_sizes, num_outputs, activation_functions, dropout_rate, regularization, lambd)\n",
    "training_losses, test_accuracies, training_accuracies, testing_losses = nn_model.train(x_train, y_train, x_test, y_test, num_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075f50d-eabe-4dc1-83d7-27132e2e41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_predictions = nn_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b66ea-52ce-4a08-a3f5-43d56a6157cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(testing_losses, label='Test Loss')\n",
    "plt.xlabel('Iterations (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Iterations (x100)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05952695-c3ca-4c48-84a6-13219c3b356f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
